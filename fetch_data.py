import yfinance as yf
import pandas as pd
import json
import time
from datetime import datetime
import os

# ğŸ” æ›¿æ¢ä¸ºä½ ä¸‹è½½å¥½çš„æˆåˆ†è‚¡åˆ—è¡¨
CSV_PATH = "data/sp500_symbols.csv"  # åŒ…å« symbol åˆ—
OUTPUT_DIR = "data"

def ensure_directory_exists(directory):
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    if not os.path.exists(directory):
        os.makedirs(directory)

def fetch_sp500_symbols_from_wiki():
    """ä»ç»´åŸºç™¾ç§‘è·å–S&P500æˆåˆ†è‚¡åˆ—è¡¨"""
    print("ğŸ“Š ä»ç»´åŸºç™¾ç§‘è·å–S&P500æˆåˆ†è‚¡åˆ—è¡¨...")
    
    # 1. ä»WikipediaæŠ“å–æ‰€æœ‰HTMLè¡¨æ ¼
    tables = pd.read_html("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies")
    
    # 2. ç¬¬ä¸€ä¸ªè¡¨æ ¼å°±æ˜¯æˆåˆ†è‚¡åˆ—è¡¨
    sp500_df = tables[0]
    
    # 3. åªå–æˆ‘ä»¬éœ€è¦çš„ä¸¤åˆ—ï¼šä»£ç  & å…¬å¸åç§°
    sp500_df = sp500_df[['Symbol', 'Security']]
    
    # 4. å»æ‰å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
    sp500_df['Symbol'] = sp500_df['Symbol'].str.replace('.', '-', regex=False).str.strip()
    
    # 5. ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼ˆä¾›æŠ“å†å²æ•°æ®ä½¿ç”¨ï¼‰
    ensure_directory_exists(OUTPUT_DIR)
    csv_path = os.path.join(OUTPUT_DIR, "sp500_symbols.csv")
    sp500_df.to_csv(csv_path, index=False)
    
    print(f"âœ… å·²ä¿å­˜æˆåˆ†è‚¡åˆ—è¡¨åˆ° {csv_path}")
    return sp500_df['Symbol'].dropna().unique().tolist()

def get_fundamentals(ticker):
    """è·å–è‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®"""
    try:
        info = ticker.info
        fundamentals = {
            # ä¼°å€¼æŒ‡æ ‡
            "pe_ratio": info.get("trailingPE"),
            "forward_pe": info.get("forwardPE"),
            "price_to_sales": info.get("priceToSalesTrailing12Months"),
            "price_to_book": info.get("priceToBook"),
            "enterprise_value": info.get("enterpriseValue"),
            "enterprise_to_revenue": info.get("enterpriseToRevenue"),
            "enterprise_to_ebitda": info.get("enterpriseToEbitda"),
            
            # å…¬å¸åŸºæœ¬ä¿¡æ¯
            "market_cap": info.get("marketCap"),
            "company_name": info.get("longName"),
            "sector": info.get("sector"),
            "industry": info.get("industry"),
            "website": info.get("website"),
            "business_summary": info.get("longBusinessSummary"),
            
            # è´¢åŠ¡æ•°æ®
            "revenue": info.get("totalRevenue"),
            "gross_profit": info.get("grossProfits"),
            "ebitda": info.get("ebitda"),
            "net_income": info.get("netIncomeToCommon"),
            
            # è‚¡æ¯æ•°æ®
            "dividend_rate": info.get("dividendRate"),
            "dividend_yield": info.get("dividendYield"),
            "payout_ratio": info.get("payoutRatio"),
            
            # æˆé•¿æ•°æ®
            "revenue_growth": info.get("revenueGrowth"),
            "earnings_growth": info.get("earningsGrowth"),
            
            # è´¢åŠ¡å¥åº·
            "debt_to_equity": info.get("debtToEquity"),
            "current_ratio": info.get("currentRatio"),
            "return_on_equity": info.get("returnOnEquity"),
            "return_on_assets": info.get("returnOnAssets")
        }
        return fundamentals
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–åŸºæœ¬é¢æ•°æ®: {e}")
        return {}

def get_financial_statements(ticker):
    """è·å–è´¢åŠ¡æŠ¥è¡¨æ•°æ®"""
    try:
        financials = {
            "income_statement": ticker.income_stmt.to_dict() if hasattr(ticker, 'income_stmt') and ticker.income_stmt is not None else {},
            "balance_sheet": ticker.balance_sheet.to_dict() if hasattr(ticker, 'balance_sheet') and ticker.balance_sheet is not None else {},
            "cash_flow": ticker.cashflow.to_dict() if hasattr(ticker, 'cashflow') and ticker.cashflow is not None else {}
        }
        
        # å°†æ—¥æœŸç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        for statement_type, statement_data in financials.items():
            if statement_data:
                # è½¬æ¢å¤æ‚çš„åµŒå¥—å­—å…¸ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
                serializable_data = {}
                for date, metrics in statement_data.items():
                    date_str = str(date)
                    serializable_data[date_str] = {}
                    for metric, value in metrics.items():
                        serializable_data[date_str][metric] = float(value) if pd.notnull(value) else None
                financials[statement_type] = serializable_data
                
        return financials
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–è´¢åŠ¡æŠ¥è¡¨: {e}")
        return {
            "income_statement": {},
            "balance_sheet": {},
            "cash_flow": {}
        }

def get_analyst_data(ticker):
    """è·å–åˆ†æå¸ˆé¢„æµ‹å’Œç›®æ ‡ä»·æ•°æ®"""
    try:
        recommendations = ticker.recommendations
        rec_dict = {}

        if recommendations is not None and not recommendations.empty:
            for index, row in recommendations.iterrows():
                try:
                    # å®‰å…¨è½¬æ¢æ—¥æœŸ
                    date_str = str(index.date()) if hasattr(index, 'date') else str(index)
                    rec_dict[date_str] = {
                        "firm": row.get("Firm", ""),
                        "to_grade": row.get("To Grade", ""),
                        "from_grade": row.get("From Grade", ""),
                        "action": row.get("Action", "")
                    }
                except Exception as e:
                    print(f"    âš ï¸ è·³è¿‡æ— æ•ˆæ¨èæ•°æ®: {e}")
                    continue

            return {
                "recommendations": rec_dict,
                "target_price": ticker.info.get("targetMeanPrice"),
                "target_high": ticker.info.get("targetHighPrice"),
                "target_low": ticker.info.get("targetLowPrice")
            }

        return {}

    except Exception as e:
        print(f"  âš ï¸ æ— æ³•è·å–åˆ†æå¸ˆæ•°æ®: {e}")
        return {}


def fetch_and_save():
    """è·å–å¹¶ä¿å­˜S&P500è‚¡ç¥¨çš„å†å²ä»·æ ¼å’ŒåŸºæœ¬é¢æ•°æ®"""
    ensure_directory_exists(OUTPUT_DIR)
    
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»ç»´åŸºç™¾ç§‘è·å–
    if not os.path.exists(CSV_PATH):
        symbols = fetch_sp500_symbols_from_wiki()
    else:
        df_symbols = pd.read_csv(CSV_PATH)
        symbols = df_symbols['Symbol'].dropna().unique().tolist()

    price_data = {}  # å†å²ä»·æ ¼æ•°æ®
    fundamental_data = {}  # åŸºæœ¬é¢æ•°æ®
    financial_statements = {}  # è´¢åŠ¡æŠ¥è¡¨
    analyst_data = {}  # åˆ†æå¸ˆæ•°æ®
    
    total_symbols = len(symbols)
    success_count = 0
    fail_count = 0
    
    print(f"ğŸš€ å¼€å§‹è·å–{total_symbols}åªè‚¡ç¥¨çš„æ•°æ®...")
    
    for i, symbol in enumerate(symbols):
        print(f"[{i+1}/{total_symbols}] å¤„ç†: {symbol}")
        try:
            ticker = yf.Ticker(symbol)
            
            # 1. è·å–å†å²ä»·æ ¼æ•°æ®
            print(f"  ğŸ“ˆ è·å–{symbol}çš„å†å²ä»·æ ¼...")
            df = ticker.history(period='max', interval='1d')
            if not df.empty:
                df.reset_index(inplace=True)
                price_data[symbol] = [
                    {"date": str(row['Date'].date()), 
                     "open": round(row['Open'], 2) if not pd.isna(row['Open']) else None,
                     "high": round(row['High'], 2) if not pd.isna(row['High']) else None,
                     "low": round(row['Low'], 2) if not pd.isna(row['Low']) else None,
                     "close": round(row['Close'], 2) if not pd.isna(row['Close']) else None,
                     "volume": int(row['Volume']) if not pd.isna(row['Volume']) else None}
                    for _, row in df.iterrows()
                ]
            
            # 2. è·å–åŸºæœ¬é¢æ•°æ®
            print(f"  ğŸ“Š è·å–{symbol}çš„åŸºæœ¬é¢æ•°æ®...")
            fundamental_data[symbol] = get_fundamentals(ticker)
            
            # 3. è·å–è´¢åŠ¡æŠ¥è¡¨
            print(f"  ğŸ“‘ è·å–{symbol}çš„è´¢åŠ¡æŠ¥è¡¨...")
            financial_statements[symbol] = get_financial_statements(ticker)
            
            # 4. è·å–åˆ†æå¸ˆæ•°æ®
            print(f"  ğŸ‘¨â€ğŸ’¼ è·å–{symbol}çš„åˆ†æå¸ˆé¢„æµ‹...")
            analyst_data[symbol] = get_analyst_data(ticker)
            
            success_count += 1
            # æ¯å¤„ç†ä¸€ä¸ªè¯·æ±‚æš‚åœä¸€æ®µæ—¶é—´ï¼Œé¿å…è¢«é™åˆ¶
            time.sleep(1.5)
            
        except Exception as e:
            print(f"âŒ è·å–{symbol}æ•°æ®å¤±è´¥: {e}")
            fail_count += 1
            time.sleep(1)  # å‘ç”Ÿé”™è¯¯åçŸ­æš‚æš‚åœ

    # ä¿å­˜å„ç±»æ•°æ®åˆ°ä¸åŒæ–‡ä»¶
    data_files = {
        "price_data": os.path.join(OUTPUT_DIR, "sp500_prices.json"),
        "fundamental_data": os.path.join(OUTPUT_DIR, "sp500_fundamentals.json"),
        "financial_statements": os.path.join(OUTPUT_DIR, "sp500_financials.json"),
        "analyst_data": os.path.join(OUTPUT_DIR, "sp500_analyst.json")
    }
    
    for data_type, file_path in data_files.items():
        with open(file_path, "w") as f:
            json.dump(eval(data_type), f)
        print(f"âœ… å·²ä¿å­˜{data_type}åˆ° {file_path}")
    
    # ä¿å­˜å®Œæ•´æ•°æ®ï¼ˆå¯é€‰ï¼Œæ•°æ®é‡å¯èƒ½å¾ˆå¤§ï¼‰
    all_data = {
        "price_data": price_data,
        "fundamental_data": fundamental_data,
        "financial_statements": financial_statements,
        "analyst_data": analyst_data,
        "metadata": {
            "update_time": str(datetime.now()),
            "total_symbols": total_symbols,
            "success_count": success_count,
            "fail_count": fail_count
        }
    }
    
    with open(os.path.join(OUTPUT_DIR, "sp500_all_data.json"), "w") as f:
        json.dump(all_data, f)
    
    print(f"âœ… æ•°æ®æ›´æ–°å®Œæˆ")
    print(f"ğŸ¯ æˆåŠŸè·å–: {success_count}/{total_symbols} è‚¡ç¥¨æ•°æ®")
    print(f"â›” å¤±è´¥: {fail_count}/{total_symbols}")
    print(f"ğŸ•’ æ›´æ–°æ—¶é—´: {datetime.now()}")
    
    return all_data

if __name__ == "__main__":
    fetch_and_save()